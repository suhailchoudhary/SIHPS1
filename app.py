# -*- coding: utf-8 -*-
"""Chatbot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NPXuGK2S1jNX3L-ZVxYrSHooLktidvy9
"""

!pip install fastapi "uvicorn[standard]" pyngrok nest_asyncio transformers peft torch bitsandbytes
!ngrok authtoken 2vcHiEWaejgZVRUn8NUDnhkE6uu_6oHqTgenn4xBpg9d3Z8p9



import zipfile
import nest_asyncio
from pyngrok import ngrok
import uvicorn
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from peft import PeftModel
import asyncio

zip_path = "redpajama-marine-final.zip"
extract_to = "./redpajama-marine-final"
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_to)
print(f"Extracted to {extract_to}")

# ---------------------------
# Fix asyncio in Colab
# ---------------------------
nest_asyncio.apply()

# ---------------------------
# Create FastAPI app
# ---------------------------
app = FastAPI()
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ---------------------------
# Load model
# ---------------------------
MODEL_DIR = "./redpajama-marine-final"
tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR, local_files_only=True)
tokenizer.pad_token = tokenizer.eos_token

quant_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True
)

base_model = AutoModelForCausalLM.from_pretrained(
    "microsoft/phi-2",
    quantization_config=quant_config,
    device_map="auto",
    trust_remote_code=True
)
model = PeftModel.from_pretrained(base_model, MODEL_DIR)
model.eval()

# ---------------------------
# API endpoint
# ---------------------------
class Query(BaseModel):
    question: str

@app.post("/chat")
def chat_endpoint(query: Query):
    prompt = f"<|user|> {query.question} <|end|>\n<|assistant|> "
    inputs = tokenizer(prompt, return_tensors="pt").to("cuda")

    outputs = model.generate(
        **inputs,
        max_new_tokens=250,
        do_sample=True,
        temperature=0.6,
        top_k=50,
        top_p=0.9,
        repetition_penalty=1.1,
        pad_token_id=tokenizer.eos_token_id
    )
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    response = response.replace(prompt, "").replace("<|end|>", "").strip()
    return {"answer": response}

# ---------------------------
# Start ngrok
# ---------------------------
PORT = 8050
public_url = ngrok.connect(PORT)
print("Public URL:", public_url)


# ---------------------------
# Run Uvicorn using Server
# ---------------------------
config = uvicorn.Config(app, host="0.0.0.0", port=PORT, log_level="info")
server = uvicorn.Server(config)
asyncio.get_event_loop().run_until_complete(server.serve())

